{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burrows's Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate Burrows's Delta using 4 most frequent words (MFWs) and 3 samples (Gratian[012].txt). That keeps the scale of the example manageable -- we can show all the intermediate results, so the reader can follow along. 4 MFWs makes the point that we are not limited to the 3 dimensions the human mind can visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3,605 words in the hypothetical cases statements (*themata*), 66,238 words in the first-recension *dicta* (including *de Pen.*), and 14,811 words in the second-recension *dicta* (including *de Pen.*) This represents the 84,654 word corpus of extended text attributable to Gratian. (The qualifier *extended* indicating that the samples do not include the short caption-like rubrics that Gratian prepended to the canons.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(filename):\n",
    "    '''open text file and return list of tokens'''\n",
    "    text = open(filename, 'r').read().lower()\n",
    "    tokens = [word for word in re.split('\\W', text) if word != '']\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(tokenize('Gratian0.txt'))": "3605",
     "len(tokenize('Gratian1.txt'))": "66238",
     "len(tokenize('Gratian2.txt'))": "14811"
    }
   },
   "source": [
    "There are {{len(tokenize('Gratian0.txt'))}} words in the hypothetical cases statements (*themata*), {{len(tokenize('Gratian1.txt'))}} words in the first-recension *dicta* (including *de Pen*.), and {{len(tokenize('Gratian2.txt'))}} words in the second-recension *dicta* (including *de Pen*.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There are 3605 words in the hypothetical cases statements\n",
       "    (*themata*), 66238 words in the first-recension *dicta* (including\n",
       "    *de Pen*.), and 14811 words in the second-recension *dicta*\n",
       "    (including *de Pen*.).  This represents the 84654 word corpus of\n",
       "    extended text attributable to Gratian.  (The qualifier *extended*\n",
       "    indicating that the samples do not include the short caption-like\n",
       "    rubrics that Gratian prepended to the \n",
       "    canons.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "g0 = len(tokenize('Gratian0.txt'))\n",
    "g1 = len(tokenize('Gratian1.txt'))\n",
    "g2 = len(tokenize('Gratian2.txt'))\n",
    "md(\n",
    "    '''There are {} words in the hypothetical cases statements\n",
    "    (*themata*), {} words in the first-recension *dicta* (including\n",
    "    *de Pen*.), and {} words in the second-recension *dicta*\n",
    "    (including *de Pen*.).  This represents the {} word corpus of\n",
    "    extended text attributable to Gratian.  (The qualifier *extended*\n",
    "    indicating that the samples do not include the short caption-like\n",
    "    rubrics that Gratian prepended to the \n",
    "    canons.)'''.format(g0, g1, g2, g0 + g1 +g2)\n",
    ")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract lists of the 4 most frequent words (MFWs) from the sample files Gratian[012].txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'non', 'et', 'est']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def occurrences(tokens):\n",
    "    '''create and return token occurrence dictionary'''\n",
    "    types = list(set(tokens))\n",
    "    tmp = dict.fromkeys(types, 0)\n",
    "    for token in tokens: tmp[token] += 1\n",
    "    occurrences = {\n",
    "        key: value for key, value in sorted(tmp.items(),\n",
    "        key = lambda item: (-item[1], item[0]))\n",
    "    }\n",
    "    return occurrences\n",
    "\n",
    "def features(texts, n):\n",
    "    '''\n",
    "    Assemble a large corpus made up of texts written by an arbitrary\n",
    "    number of authors; let’s say that number of authors is x.\n",
    "    '''\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        corpus += tokenize(text + '.txt')\n",
    "    '''\n",
    "    Find the n most frequent words in the corpus to use as features.\n",
    "    '''\n",
    "    features = list(occurrences(corpus).keys())[:n]\n",
    "    return features\n",
    "\n",
    "samples = ['Gratian1', 'Gratian2']\n",
    "features(samples, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'non', 'et', 'est']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian0', 'Gratian1'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'et', 'non', 'unde']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian0', 'Gratian2'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'non', 'et', 'est']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian1', 'Gratian2'], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you get different lists of MFWs from different possible combinations or permutations of the sample files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gratian1': {'in': 25.393278782571937,\n",
       "  'non': 24.487454331350584,\n",
       "  'et': 23.279688396388778,\n",
       "  'est': 17.150276276457625},\n",
       " 'Gratian2': {'in': 29.099993248261427,\n",
       "  'non': 21.200459118222945,\n",
       "  'et': 24.036189318749578,\n",
       "  'est': 12.018094659374789}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frequencies(features, subcorpora):\n",
    "    '''\n",
    "    For each of these n features, calculate the share of each of\n",
    "    the x authors’ subcorpora represented by this feature, as a\n",
    "    percentage of the total number of words.\n",
    "    '''\n",
    "    frequencies = {}\n",
    "    empty = dict.fromkeys(features, 0)\n",
    "    for subcorpus in subcorpora:\n",
    "        frequencies[subcorpus] = empty.copy()\n",
    "        tokens = tokenize(subcorpus + '.txt')\n",
    "        counts = occurrences(tokens)\n",
    "        for feature in features:\n",
    "            frequencies[subcorpus][feature] = (counts.get(feature, 0) / len(tokens)) * 1000\n",
    "            # frequencies[subcorpus][feature] = (counts.get(feature, 0))\n",
    "    return frequencies\n",
    "\n",
    "mfws = features(samples, 4)\n",
    "frequencies(mfws, samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s=\\sqrt{\\frac{1}{N - 1}\\sum_{i=1}^N(x_i-\\bar{x})^2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gratian1': {'in': 25.393278782571937,\n",
       "  'non': 24.487454331350584,\n",
       "  'et': 23.279688396388778,\n",
       "  'est': 17.150276276457625},\n",
       " 'Gratian2': {'in': 29.099993248261427,\n",
       "  'non': 21.200459118222945,\n",
       "  'et': 24.036189318749578,\n",
       "  'est': 12.018094659374789},\n",
       " 'means': {'in': 27.246636015416684,\n",
       "  'non': 22.843956724786764,\n",
       "  'et': 23.657938857569178,\n",
       "  'est': 14.584185467916207},\n",
       " 'stdevs': {'in': 2.621042934611309,\n",
       "  'non': 2.324256604930275,\n",
       "  'et': 0.5349269321751997,\n",
       "  'est': 3.6290004237202145}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def means(frequencies):\n",
    "    '''\n",
    "    Then, calculate the mean and the standard deviation of these x\n",
    "    values and use them as the offical mean and standard deviation\n",
    "    for this feature over the whole corpus. In other words, we will\n",
    "    be using a mean of means instead of calculating a single value\n",
    "    representing the share of the entire corpus represented by each\n",
    "    word.\n",
    "    '''\n",
    "    subcorpora = list(frequencies.keys())\n",
    "    features = list(frequencies[subcorpora[0]].keys())\n",
    "    frequencies['means'] = dict.fromkeys(features, 0)\n",
    "    frequencies['stdevs'] = dict.fromkeys(features, 0)\n",
    "    for feature in features:\n",
    "        frequency_list = []\n",
    "        for subcorpus in subcorpora:\n",
    "            frequency_list.append(frequencies[subcorpus][feature])\n",
    "        frequencies['means'][feature] = statistics.mean(frequency_list)\n",
    "        frequencies['stdevs'][feature] = statistics.stdev(frequency_list)\n",
    "    return frequencies\n",
    "\n",
    "means(frequencies(mfws, samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gratian1': {'in': -0.7071067811865482,\n",
       "  'non': 0.7071067811865475,\n",
       "  'et': -0.7071067811865475,\n",
       "  'est': 0.7071067811865475},\n",
       " 'Gratian2': {'in': 0.7071067811865468,\n",
       "  'non': -0.7071067811865475,\n",
       "  'et': 0.7071067811865475,\n",
       "  'est': -0.7071067811865475}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def z_scores(subcorpora, frequencies):\n",
    "    '''\n",
    "    For each of the n features and x subcorpora, calculate a z-score\n",
    "    describing how far away from the corpus norm the usage of this\n",
    "    particular feature in this particular subcorpus happens to be.\n",
    "    To do this, subtract the \"mean of means\" for the feature from\n",
    "    the feature’s frequency in the subcorpus and divide the result\n",
    "    by the feature’s standard deviation.\n",
    "    '''\n",
    "    z_scores = {}\n",
    "    features = list(frequencies[subcorpora[0]].keys())\n",
    "    for subcorpus in subcorpora:\n",
    "        z_scores[subcorpus] = dict.fromkeys(features, 0)\n",
    "        for feature in features:\n",
    "            z_scores[subcorpus][feature] = (frequencies[subcorpus][feature] - frequencies['means'][feature]) / frequencies['stdevs'][feature]\n",
    "    return z_scores\n",
    "\n",
    "z_scores(samples, means(frequencies(mfws, samples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testcase(test, features, frequencies, z_scores):\n",
    "    test_tokens = []\n",
    "    test_tokens = tokenize(test + '.txt')\n",
    "    test_frequencies = occurrences(test_tokens)\n",
    "    frequencies[test] = dict.fromkeys(features, 0)\n",
    "    z_scores[test] = dict.fromkeys(features, 0)\n",
    "    for feature in features:\n",
    "       frequencies[test][feature] = (test_frequencies.get(feature, 0) / len(test_tokens)) * 1000\n",
    "       z_scores[test][feature] = (frequencies[test][feature] - frequencies['means'][feature]) / frequencies['stdevs'][feature]\n",
    "    return (frequencies, z_scores)\n",
    "\n",
    "f, z = testcase('Gratian0', mfws, means(frequencies(mfws, samples)), z_scores(samples, means(frequencies(mfws, samples))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
