{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burrows's Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humanum genus duobus regitur\n"
     ]
    }
   ],
   "source": [
    "print('Humanum genus duobus regitur')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\mu)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate Burrows's Delta using 4 most frequent words (MFWs) and 3 samples (Gratian[012].txt). That keeps the scale of the example manageable -- we can show all the intermediate results, so the reader can follow along. 4 MFWs makes the point that we are not limited to the 3 dimensions the human mind can visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that we're using the correct versions of the sample files by verifying their SHA1 cryptographic checksums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "assert(hashlib.sha1(open('Gratian0.txt', 'rb').read()).hexdigest() == '80b33a5ede195049ebb39d76a9b52c9314be2452')\n",
    "assert(hashlib.sha1(open('Gratian1.txt', 'rb').read()).hexdigest() == '5dc6dd1a83efbf85ea63e15539f1714db43bb84c')\n",
    "assert(hashlib.sha1(open('Gratian2.txt', 'rb').read()).hexdigest() == '62c0e13d63466caafa343bc5ece76fb2f24a8697')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify word counts. There are 3,605 words in the hypothetical cases statements (*themata*), 66,238 words in the first-recension *dicta* (including *de Pen.*), and 14,811 words in the second-recension *dicta* (including *de Pen.*) This represents the 84,654 word corpus of extended text attributable to Gratian. (The qualifier *extended* indicating that the samples do not include the short caption-like rubrics that Gratian prepended to the canons.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(filename):\n",
    "    '''open text file and return list of tokens'''\n",
    "    text = open(filename, 'r').read().lower()\n",
    "    tokens = [word for word in re.split('\\W', text) if word != '']\n",
    "    return tokens\n",
    "assert(len(tokenize('Gratian0.txt')) == 3605)\n",
    "assert(len(tokenize('Gratian1.txt')) == 66238)\n",
    "assert(len(tokenize('Gratian2.txt')) == 14811)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract lists of the 4 most frequent words (MFWs) from the sample files Gratian[012].txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 2187), ('et', 1968), ('non', 1960), ('est', 1327)]\n",
      "[('in', 1756), ('non', 1646), ('et', 1612), ('est', 1149)]\n",
      "[('in', 505), ('et', 426), ('non', 338), ('unde', 234)]\n",
      "[('in', 2113), ('non', 1936), ('et', 1898), ('est', 1314)]\n"
     ]
    }
   ],
   "source": [
    "def frequencies(tokens):\n",
    "    '''create and return token frequency dictionary'''\n",
    "    types = list(set(tokens))\n",
    "    tmp = dict.fromkeys(types, 0)\n",
    "    for token in tokens: tmp[token] += 1\n",
    "    token_frequencies = {\n",
    "        key: value for key, value in sorted(tmp.items(),\n",
    "        key = lambda item: (-item[1], item[0]))\n",
    "    }\n",
    "    return token_frequencies\n",
    "\n",
    "def get_features(texts, n):\n",
    "    '''\n",
    "    Assemble a large corpus made up of texts written by an arbitrary\n",
    "    number of authors; letâ€™s say that number of authors is x.\n",
    "    '''\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        corpus += tokenize(text + '.txt')\n",
    "    '''\n",
    "    Find the n most frequent words in the corpus to use as features.\n",
    "    '''\n",
    "    features = list(frequencies(corpus).items())[:n]\n",
    "    return features\n",
    "\n",
    "print(get_features(['Gratian0', 'Gratian1', 'Gratian2'], 4))\n",
    "print(get_features(['Gratian0', 'Gratian1'], 4))\n",
    "print(get_features(['Gratian0', 'Gratian2'], 4))\n",
    "print(get_features(['Gratian1', 'Gratian2'], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|in|non|et|est|\n",
    "|:-|:-|:-|:-|\n",
    "|2113|1936|1898|1314|\n",
    "\n",
    "Note that you get different lists of MFWs from different possible combinations or permutations of the sample files!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
