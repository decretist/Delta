{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burrows's Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(x_i-\\mu)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate Burrows's Delta using 4 most frequent words (MFWs) and 3 samples (Gratian[012].txt). That keeps the scale of the example manageable -- we can show all the intermediate results, so the reader can follow along. 4 MFWs makes the point that we are not limited to the 3 dimensions the human mind can visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that we're using the correct versions of the sample files by verifying their SHA1 cryptographic checksums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "assert(hashlib.sha1(open('Gratian0.txt', 'rb').read()).hexdigest() == '80b33a5ede195049ebb39d76a9b52c9314be2452')\n",
    "assert(hashlib.sha1(open('Gratian1.txt', 'rb').read()).hexdigest() == '5dc6dd1a83efbf85ea63e15539f1714db43bb84c')\n",
    "assert(hashlib.sha1(open('Gratian2.txt', 'rb').read()).hexdigest() == '62c0e13d63466caafa343bc5ece76fb2f24a8697')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify word counts. There are 3,605 words in the hypothetical cases statements (*themata*), 66,238 words in the first-recension *dicta* (including *de Pen.*), and 14,811 words in the second-recension *dicta* (including *de Pen.*) This represents the 84,654 word corpus of extended text attributable to Gratian. (The qualifier *extended* indicating that the samples do not include the short caption-like rubrics that Gratian prepended to the canons.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(filename):\n",
    "    '''open text file and return list of tokens'''\n",
    "    text = open(filename, 'r').read().lower()\n",
    "    tokens = [word for word in re.split('\\W', text) if word != '']\n",
    "    return tokens\n",
    "assert(len(tokenize('Gratian0.txt')) == 3605)\n",
    "assert(len(tokenize('Gratian1.txt')) == 66238)\n",
    "assert(len(tokenize('Gratian2.txt')) == 14811)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "len(tokenize('Gratian0.txt'))": "3605",
     "len(tokenize('Gratian1.txt'))": "66238",
     "len(tokenize('Gratian2.txt'))": "14811"
    }
   },
   "source": [
    "There are {{len(tokenize('Gratian0.txt'))}} words in the hypothetical cases statements (themata), {{len(tokenize('Gratian1.txt'))}} words in the first-recension dicta (including de Pen.), and {{len(tokenize('Gratian2.txt'))}} words in the second-recension dicta (including de Pen.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown as md\n",
    "md('There are {} words in the hypothetical cases statements (themata).'.format(len(tokenize('Gratian0.txt'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract lists of the 4 most frequent words (MFWs) from the sample files Gratian[012].txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'et', 'non', 'est']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def occurrences(tokens):\n",
    "    '''create and return token occurrence dictionary'''\n",
    "    types = list(set(tokens))\n",
    "    tmp = dict.fromkeys(types, 0)\n",
    "    for token in tokens: tmp[token] += 1\n",
    "    occurrences = {\n",
    "        key: value for key, value in sorted(tmp.items(),\n",
    "        key = lambda item: (-item[1], item[0]))\n",
    "    }\n",
    "    return occurrences\n",
    "\n",
    "def features(texts, n):\n",
    "    '''\n",
    "    Assemble a large corpus made up of texts written by an arbitrary\n",
    "    number of authors; let’s say that number of authors is x.\n",
    "    '''\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        corpus += tokenize(text + '.txt')\n",
    "    '''\n",
    "    Find the n most frequent words in the corpus to use as features.\n",
    "    '''\n",
    "    features = list(occurrences(corpus).keys())[:n]\n",
    "    return features\n",
    "\n",
    "features(['Gratian0', 'Gratian1', 'Gratian2'], 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'non', 'et', 'est']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian0', 'Gratian1'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'et', 'non', 'unde']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian0', 'Gratian2'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'non', 'et', 'est']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(['Gratian1', 'Gratian2'], 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you get different lists of MFWs from different possible combinations or permutations of the sample files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|in|non|et|est|\n",
    "|:-|:-|:-|:-|\n",
    "|2113|1936|1898|1314|\n",
    "\n",
    "Note that you get different lists of MFWs from different possible combinations or permutations of the sample files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1682\r\n"
     ]
    }
   ],
   "source": [
    "!cat Gratian1.txt | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | tr '[:space:]' '\\n' | egrep -v '^$' | egrep \"\\bin\\b\" | wc -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gratian0': {'in': 20.527045769764214,\n",
       "  'et': 19.41747572815534,\n",
       "  'non': 6.65742024965326,\n",
       "  'est': 3.6061026352288486,\n",
       "  'unde': 0.0},\n",
       " 'Gratian1': {'in': 25.393278782571937,\n",
       "  'et': 23.279688396388778,\n",
       "  'non': 24.487454331350584,\n",
       "  'est': 17.150276276457625,\n",
       "  'unde': 7.518342945137232},\n",
       " 'Gratian2': {'in': 29.099993248261427,\n",
       "  'et': 24.036189318749578,\n",
       "  'non': 21.200459118222945,\n",
       "  'est': 12.018094659374789,\n",
       "  'unde': 15.79906826007697}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frequencies(features, subcorpora):\n",
    "    '''\n",
    "    For each of these n features, calculate the share of each of\n",
    "    the x authors’ subcorpora represented by this feature, as a\n",
    "    percentage of the total number of words.\n",
    "    '''\n",
    "    frequencies = {}\n",
    "    empty = dict.fromkeys(features, 0)\n",
    "    for subcorpus in subcorpora:\n",
    "        frequencies[subcorpus] = empty.copy()\n",
    "        tokens = tokenize(subcorpus + '.txt')\n",
    "        counts = occurrences(tokens)\n",
    "        for feature in features:\n",
    "            frequencies[subcorpus][feature] = (counts.get(feature, 0) / len(tokens)) * 1000\n",
    "            # frequencies[subcorpus][feature] = (counts.get(feature, 0))\n",
    "    return frequencies\n",
    "\n",
    "frequencies(['in', 'et', 'non', 'est', 'unde'], ['Gratian0', 'Gratian1', 'Gratian2'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
